# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
1. Подготовил слайсы данных (1000, 10000 и 100000 записией).
2. Написал бенчмарк скрипт, который позволяет подсчитать время выполнения с учетом переданных данных.
3. Написал профайлер скрипт, используя ruby-prof, позволяющий понять точку роста.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~23 секунды.

Вот как я построил `feedback_loop`:
1. Запускал профайлер, строил отчет, находил основную точку роста.
2. Вносил в код изменения, чтобы сократить время выполнения программы в основной точке роста.
3. Запускал тест, чтобы убедиться, что программа работает корректно.
4. Запускал профайлер, чтобы убедиться, что внесенные изменения сокращают время работы программы.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof.

Вот какие проблемы удалось найти и решить

### Излишняя итерация и создание массивов
В ходе профилирования слайса 10000 строк была выявлена проблема излишнего создания массива сессий, по
которому в последствии происходил поиск сессий конкретного пользователя путем полного перебора массива в массиве O(n2). Было принято сохранять сессии к нужному пользователю, что позволило избжеть тяжелого `select'a`
для каждого пользователя.

### Излишняя итерация по массиву для формаирования окончательных данных для каждого пользователя
В финальном отчете для каждого пользователя формируется статистика по его сессиям. В изначальном
скрипте это статистика генерировалась не оптимальным путем, т.е. для каждого показателя была
полная итерация по массиву пользователей, что так же приводило к ухудшению производительности.

### Неоптимальная сериализация
В изначальном скрипте для сериализации Hash в JSON использовалась стандартная библиотека JSON,
которая работает в ~10 раз медленее, чем [Oj](https://github.com/ohler55/oj)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с более 4-х часов (дождаться не смог) до 22-25 секунд и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан
тест на rspec, который проверяет, что программа работает с исходным файлом менее, чем за 30 секунду, что является заданным бюджетом. При необходимости тест можно отключить:
```bash
bundle exec rspec --tag ~@slow
```