# Case-study оптимизации

## Актуальная проблема
В проекте возникла проблема: программа для обработки данных на ruby успешно работала с файлами до 1-2 мегабайт, но была непригодна для использования на данных большого объёма. С файлом около 100 мегабайт было непонятно, закончит ли она вообще работу за какое-то разумное время.

Я решила исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я использовала метрику «Время обработки файла на 5000 строк ~ 200 кБ». Такой объём данных обрабатывался достаточно быстро, чтобы выстроить эффективный фидбек-луп, и в то же время был достаточно показательным.

| rows | t, sec |
|------|--------|
| 5000 | ~1.50  |

Эту метрику я зафиксировала с помощью теста RSpec::Benchmark.

Также я дополнительно зафиксировала начальную асимптотику времени работы — при увеличении объёма данных в 2 раза время работы программы увеличивалось в 4 раза.

|    | rows | t     |
|----|------|-------|
| x  | 1000 | ~0.06 |
| 2x | 2000 | ~0.24 |
| 4x | 4000 | ~0.94 |
| 8x | 8000 | ~3.72 |

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~ 25 секунд.

Вот как я построил `feedback_loop`:
→ Профилирование (RubyProf, GraphHtmlPrinter)                 ~ 2 sec
→ Изменения кода
→ Тестирование работоспособности программы (MiniTest)         ~ 1 sec
→ Тестирование метрики (не деградировала) (RSpec::Benchmark)  ~ 20 sec
→ Benchmark                                                   ~ 2 sec

## Вникаем в детали системы, чтобы найти главные точки роста
Основной инструмент, которй я использовала, чтобы найти "точки роста" для оптимизации — RubyProf (отчёт GraphHtml). Чтобы отчёт был показательным, я перенесла основные действия программы в именованные методы.

Вот какие проблемы удалось найти и решить.

### 82% времени программа выбирает сессии юзера из общего массива сессий.
Проблема видна из отчёта Graph RubyProf.

Чтобы ускорить выборку сессий для пользователя, уже при парсинге сессии должны раскладываться не в массив, а в хэш, где ключами будут айдишники юзеров.

Изменения в метрике:
| rows | t, sec |
|------|--------|
| 5000 | ~0.175 |

Программа стала работать быстрее почти в 10 раз. Теперь на выбор сессий юзера тратится 0.29% общего времени работы программы.

### 23.74%	времени программа парсит даты и сортирует их
Проблема видна из отчёта Graph RubyProf. При взгляде на исходные данные видно, что оперировать датами можно как простыми строками — они уже даются в нужном формате. Убираем парсинг дат и конвертацию.

Изменения в метрике:
| rows | t, sec |
|------|--------|
| 5000 | ~0.130 |

Программа стала быстрее ещё на ~25%, а сортировка дат занимает 0.10% от общего времени работы программы.

### 42.2% времени программы занимает Enumerable#all?, а точнее его callee Array#each
Проблема видна из отчёта Graph RubyProf.
Критичное количество раз метод all? вызывается при добавлении нового элемента в массив уникальных браузеров. Вместо того, чтобы проверять массив каждый раз, кладём в него всё подряд, потом уникализируем один раз сразу всё.

Изменения в метрике:
| rows | t, sec |
|------|--------|
| 5000 | ~0.125 |

Enumerable#all? стал занимать 1,5% от общего времени программы. Производительность программы увеличилась.

### 15%	отнимает метод Array#map	во время сборки отчёта.
Снова возвращаемся к парсеру сессий из файла и раскладывам всё в хэши вместо массивов. Заодно при парсинге сессий формируем отдельный массив из всех браузеров для общего отчёта и считаем все сессии.

Изменения в метрике:
| rows | t, sec |
|------|--------|
| 5000 | ~0.08  |

 +35% к скорости.
Последний штрих — избавляемся от избыточного #split и пкрасивенько расставляем оптимизированные методы :)

| rows | t, sec |
|------|--------|
| 5000 | ~0.047 |

Ещё +50%.

### JSON::Ext::Generator::GeneratorMethods::Hash#to_json
Меняем json на oj.

| rows | t, sec |
|------|--------|
| 5000 | ~0.028 |

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 1,5 секунд до 0,034 секунд (~ в 32 раза) и уложиться (почти:)) в заданный бюджет.
Обработка большого файла занимает 30-35 секунд.
Асимптотика программы стала линейной.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы было добавлено 2 теста:
1. #work with 5000-rows file works under 70 ms
2. #work has linear asymptotics

