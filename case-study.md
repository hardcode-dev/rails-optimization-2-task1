# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время обработки тестового файла, размером 20000 строк.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~10 секунд

Вот как я построил `feedback_loop`: подготовил файл, обработка которого выполняется за приемлемое время и написал тест измеряющий время обработки этого файла

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:   
- профилировщиками RBSPY и ruby-prof
- советами линтеров rubocop-performance и fasterer
- примерами более оптимальных способов работы с данными из fast-ruby
- головой

Вот какие проблемы удалось найти и решить

### Находка №1
Отключение GC приводит к увеличению времени обработки и не приводит к "стабильному" времени выполнения.
Прошелся по коду performance чекерами (rubocop-performance и fasterer), поправил замечания.

### Находка №2
Запустил профилировщик RBSPY при обработке большого файла, обратил внимание что скрипт не проходит дальше разбиения строки на пользователей и сессии. 
Внес немного оптимизаций - функция split вызывалась многократно.
На тестовых данных это улучшение почти не показало результатов.

### Находка №3
Сгенерировал Flat отчет из ruby-prof, обратил внимание что 98% времени занимает выборка сессий при формировании обьектов пользователей.
Зарефакторил эту логику - перенес создание обьекта пользователя в блок парсинга строк и стал собирать сессии в хеш по ключу user_id. 
Это помогло значительно ускорить обработку файла на 20000 строк - с ~6 секунд до ~0.5

### Находка №4
Снова сгенерировал Flat отчет, на этот раз самое значительное время (34%) заняла проверка уникальности браузера. 
Перенес сбор этой информации в блок парсинга (использовал класс Set). 
Обработка файла на 100000 строк ускорилась с ~2.2 секунд до ~1.7

### Находка №4
Сгенерировал Callstack отчет, больше всего времени (68%) занял метод `collect_stats_from_users` - этот метод вызывается несколько раз для сбора различной информации по пользователям
Собрал все в один большой метод чтобы в процессе сбора информации выполнять только один проход по пользователям
Обработка файла на 200000 строк ускорилась с ~3.5 секунд до ~3.0

### Находка №5
Время ниже - обработка файла размером 500000 строк.  
  
Пользуясь отчетами профилировщиков и примерами из fast-ruby:  
Заменил Data.parse на Data.iso8601 (формат даты в исходном файле позволяет это сделать)  
~8.5 -> ~6.5  
Конвертируем длительность сессии в Integer во время сбора сессий, что позволяет упростить выборку данных по этому параметру  
Upcase названия браузера во время сбора сессий  
Собрал все map'ы по сессиям пользователя в один each  
~6.5 -> ~5.5  
Убрал конвертацию строки даты в обьект Date - формат дат позволяет сортировать их. Возможно так было делать нельзя :)  
~5.5 -> ~4.0  

### Ваша находка №6
Время обработки исходного большого файла (3250940 строк) к данному этапу: 40 - 42 секунды  
Oj вместо .to_json + убрал класс пользователя -> ~32 секунд.  
Использовал символы вместо строк для ключей хешей; перенес затратный upcase браузеров в места вставки готовых данных в результат отчета (сравнение сделал нечуствительным к регистру) - файл стал обрабатываться 25-27 секунд


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось уменьшить время обработки файла на 20000 строк с ~6 секунд до 0.1, сделать время обработки линейным относительно величины файла и уложиться в заданный бюджет.

### Какими ещё результами можете поделиться
Progressbar сильно замедляет работу программы :)

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавил тест проверяющий линейный рост сложности, а также тест на время обработки файла размером 100000 строк.
