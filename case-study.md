# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 

Тестовый файл с данными в 135Мб должен укладываться в 30 секунд


## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 
на первом этапе, до оптимизаций я решил использовать тестовый файл размером 15000 который выполняется за 6 секунд что позволит быстро видеть результат и нивелирует влияние прогрева программы


## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался 

- запуск на различных обьемах данных чтобы понять зависимость времени работы от размера файла
- запуск на большой обьеме данных и профилировка rbspy
- запуск на небольшом обьеме с ruby-prof и qcachegrind

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- запуск на разных обьемах показал, что рост следующий 
1000 строк:  0.062604s
10_000 строк: 2.321406s
20_000 строк: 15.514703s
Зависимость явно хуже чем линейная, было решено попробовать rbspy - чем дольше времени работы тем лучше rbspy покажет проблемное место с экспоненциальным ростом
Но толку с этого не оказалось - все в c-function

### Ваша находка №2
- ruby-prof показал что основная проблема в Array.select
- как оказалось у нас всего 1 селект user_sessions = sessions.select
- так как сильно менять код вокруг не хочется, а по памяти у нас проблем нет я решил создать хэш для быстрого поиска по массиву

  hashed_sessions = sessions.group_by{ |session| session['user_id'] }
  и обновил поиск на 
  user_sessions = hashed_sessions[user['id']]
  
- 15_000 строк стали отрабатывать за 0.9с - что в 7 раз быстрее
- в отчете профилировщика основная проблема переехала в Array:each

### Ваша находка №3
- я увеличил тестовый обьем до 100_000 строк так как 15 ставли выполнятся слишком быстро
- ruby-prof Callgrind показал что проблема в Array.each вызываемом из work дальнейший анализ вкладки SourceCode показал что проблема тут 
  file_lines.each do |line|
    cols = line.split(',')
    users = users + [parse_user(line)] if cols[0] == 'user'
    sessions = sessions + [parse_session(line)] if cols[0] == 'session'
  end
- из отчета также видно что split, parse_user & parse_session занимают мало времени 
- дабы убедиться что проблема именно тут я сравнил время выполнения исходной программы и варианта где после этого array.each стоит return - время 14 и 17с подтвердило мою правоту - точка роста именно тут  
- методом комментирования я нашел что основная проблема в строке
sessions = sessions + [parse_session(line)] if cols[0] == 'session'
- так как я неопытный пользователь рубипрофа я решил проверить все ли я правильно понимаю и убрал парсинг сесии и юзера, время не изменилось
из чего я сделал вывод что проблема в том что парсинг каждой строки приводил к созданию минимум 3 новых обьектов типа массив и изменил код так
  file_lines.each do |line|
    cols = line.split(',')
    users << parse_user(line) if cols[0] == 'user'
    sessions << parse_session(line) if cols[0] == 'session'
  end
Это снизило время парсинга 100_000 строкового файла с 17 до 4c  
Я увеличил размер до 200_000 и рубипроф снова показал что проблема все еще в Array.each
Но теперь он вызывался из Enumerable#all? 169251/169256 но их два, в бой пошел GraphHtmlPrinter который показал что all вызывается из each и что collect_stats_from_users нипричем
Отлично, проблема найдена 
  uniqueBrowsers = []
  sessions.each do |session|
    browser = session['browser']
    uniqueBrowsers += [browser] if uniqueBrowsers.all? { |b| b != browser }
  end
Уникальные браузеры? Тут я придумал 2 способа как сделать быстрее но не знал какой лучше поэтому сделал бенчмарк 

  z1 = Benchmark.realtime { 20.times{ sessions.select{|session| session['browser'] }.uniq } }
  puts "test1 #{z1}"
  z2 =  Benchmark.realtime { 20.times {sessions.uniq{|session| session['browser']}.map{|session| session['browser']}} }
  puts "test2: #{z2}"

Оказалось что второй вариант намного быстрее 

test1 16.18229799999972
test2: 4.273353999999927

Потом я заметил что дальше массив не нужен так что я заменил код сразу на расчет каунт что оказалось еще быстрее
test2: 1.3805479999991803

Но в целом это особо не помогло, значит точку роста определили неправильно. 
По стате все еще 
array.each 90.58% total & 	50.90% self

Я вынес содержимое each в collect_stats_from_users в отдельную функцию fill_report_from_user там оказалось 39% времени но маленький селф
и тогда я переделал создание user_objects на collect 
 users_objects = users.collect do |user|
    attributes = user
    user_sessions = hashed_sessions[user['id']] || []

    User.new(attributes: attributes, sessions: user_sessions)
  end
тут заодно и ушло большое создание лишних массивов в сумме

Особо глобальных проблем не осталось, остались чуть поменьше, например DateParse 21%

Кстати большой файл уже парсится за 2м17с
  

### Ваша находка №4

Так как мы теперь можем распарсить большой файл за адекватное время я загнал его весь в рубипроф
22.53% <Class::Date>#parse смотрим stackoverflow меняем на strptime снизили до 5% 
Тут же сразу заметил map-map связки и map-any тоже присутствуют в ruby-prof - убираем 
map+max => max_by
map + sum => sum

И пока смотрел upcase постоянно вызывается на браузере хотя без апкейза не используется, всего 4% но заодно перенесу вызов

Так я вышел из двух минут, но последний шаг это явно было потешить самолюбие, надо искать реальные точки роста а не потакать своим желаниям



## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

