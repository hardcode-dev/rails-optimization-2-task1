# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.
Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: полное время выполнения программы.
Перед внесением изменений в программу было измерено произведен анализ асимптотики для тестовых файлов размером 2500, 5000, 10000 и 20000 строк

Файл с 2500 строками обработался за 0.22 sec
Файл с 5000 строками обработался за 0.79 sec
Файл с 10000 строками обработался за 3.07 sec
Файл с 20000 строками обработался за 12.05 sec

Отключение сборщика мусора дало падение производительности на увеличивающихся объемах данных:
Файл с 2500 строками обработался за 0.21 sec
Файл с 5000 строками обработался за 1.01 sec
Файл с 10000 строками обработался за 4.27 sec
Файл с 20000 строками обработался за 16.36 sec

Беглый анализ асимптотики показал экспоненциальный рост - примерно O(N^2)
Таким образом, исходный файл с 3_250_940 строками будет обрабатываться более 283_920 секунд (более 3 дней)

Бюджет метрики - обработать исходный файл максимум за 30 секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*
Для проверки гипотез я выбрал файл с 5000 строк как приемлемый по начальному времени обработки, но в то же время содержащий достаточное количество данных
Так же я написал тест, защищающий время обработки от регрессии, где исходным временем обработки указал 850 ms

Вот как я построил `feedback_loop`:
- произвести разовый замер времени обработки файла для получения точки отсчета
- произвести профилирование и выявить точку роста
- оптимизировать код
- произвести разовый замер времени обработки файла, чтобы убедиться, что оптимизация помогла
- выполнить тестирование корректности и скорости обработки файла
- зафиксировать новое максимальное время выполнения в тесте в случае успеха

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- ruby-prof в режиме CallStack

Вот какие проблемы удалось найти и решить:

### Находка №1
- Файл с 5000 записей обрабатывался 0.99 секунды
- Отчет показал, что точкой роста является метод `Array#select` который занимал 84.50% времени обработки файла
- Оптимизация: метод `select` выполнялся над массивом `sessions` столько раз, сколько элементов содержит массив `users`. Необходимо заменить вызов метода `select` группировкой массива `sessions` по `user_id` до перебора массива `users` и передачей внутрь перебора уже сгруппированных данных
- В результате время выполнения программы снизилось до 0.15 секунды
- Тесты корректности и скорости обработки файла были пройдены
- Максимальное время обработки файла в тесте было уменьшено до 150 ms

### Находка №2
- Файл с 5000 записей обрабатывался 0.15 секунды
- Отчет показал, что точкой роста является метод `Array#all?` который занимает 31.43% времени обработки файла
- Оптимизация: метод `all?` выполняелся над массивом `uniqueBrowsers` столько раз, сколько элементов содержит массив `sessions`. Для получения уникальных браузеров достаточно сгруппировать массив `sessions` по `browser` и преобразовать ключи полученного хеша в массив.
- В результате время выполнения программы снизилось до 0.12 секунды
- Тесты корректности и скорости обработки файла были пройдены
- Максимальное время обработки файла в тесте было уменьшено до 120 ms

### Находка №3
- Файл с 5000 записей обрабатывался 0.12 секунды
- Отчет показал, что точкой роста является метод `Array#each` который занимает 48.96% времени обработки файла. В методе происходит преобразование каждого элемента массива в отдельный массив (`split(',')`), а также передача оригинальных элементов массива в метод `parse_session` либо `parse_user`, где происходит повторное преобразование элемента в массив
- Оптимизация: передавать в дочерние методы уже преобразованные элементы, а так же заменить сложение элементов массивов `users` и `sessions` на метод добавления элементов в массив `<<`
- Избавление от дублирования метода `split` дало общий результат выполнения программы снизилось до 0.11 секунды. Замена метода добавления элементов в массив снизило время выполнения программы до 0.07 секунды
- Тесты корректности и скорости обработки файла были пройдены
- Максимальное время обработки файла в тесте было уменьшено до 75 ms

### Находка №4
- Файл с 5000 записей обрабатывался 0.07 секунды
- Отчет показал, что точкой роста является метод `Object#collect_stats_from_users ` который занимает 73.73% времени обработки файла. В методе происходит вызов метода `<Class::Date>#parse` для преобразования строкового представления даты в формат даты.
- Оптимизация: Проанализировав строку `'dates' => user.sessions.map{|s| s['date']}.map {|d| Date.parse(d)}.sort.reverse.map { |d| d.iso8601 }` я понял что происходит преобразование строкового представления даты в собственно формат даты, затем полученный массив сортируется в обратном направлении и даты снова преобразуются в строки в формате iso8601. Сделав допущение, что исходный файл для обработки выгружается какой-то системой, что значит, что формат даты у всех записей будет единым, я посмотрел на даты в исходном файле и понял, что они уже соответствуют формату iso8601. Соответственно, решением было убрать излишние преобразования даты и ее парсинг
- В результате время выполнения программы снизилось до 0.04 секунды
- Тесты корректности и скорости обработки файла были пройдены
- Максимальное время обработки файла в тесте было уменьшено до 45 ms

### Находка №5
- Файл с 5000 записей обрабатывался 0.04 секунды
- Отчет показал, что точкой роста снова является метод `Object#collect_stats_from_users ` который занимает 57.99% времени обработки файла. В методе происходит множество выховов вызов метода `collect_stats_from_users` и `Array#map`.
- Оптимизация: Было решено объединить все вызовы `collect_stats_from_users` в один и попытаться уменьшить количество вызываемых методов `map`. Для этого вызовы методов `upcase` и `to_i` были подняты до уровня подготовки данных в методе `parse_session`. Так же был оптимизирован способ смерживания хеша `report['usersStats'][user_key]` внутри метода `collect_stats_from_users`
- В результате время выполнения программы осталось прежним для файла в 5000 строк, но снизилось до 35.78 секунды для файла `data_large.txt`
- Тесты корректности и скорости обработки файла были пройдены
- Файл в тесте производительности был заменен на файл с 500_000 строк Максимальное время обработки файла в тесте было выставлено в 3000 ms

### Результаты
После всех изменений программы удалось добиться обработки полного файла за 86 секунд вместо более чем часа,
для файла с 20000 строками скорость выполнения программы увеличилась в 70 раз,
замеры показали, что относительный рост скорости тем больше, чем больше файл, т.е. для полного файла время обработки могло уменьшиться в 100 раз
