# Оптимизация процесса создания отчета

## Проблема
В проекте существует задача по формированию отчетов из файлов в формате `.txt` определенного формата  

Отчеты должны создержать информацию об общем количестве пользователей,количестве сессий, 
количестве уникальных браузеров, а также персональную статистику по каждому пользователю.  

Написанная программа хорошо работает на небольших объемах входных данных, однако слишком медленно
работает на большом объеме входных данных.  

Проблема возникла при попытке обработать файл объемом ~ 130 МБ  
В результате возникшей проблемы было решено оптимизировать процесс создания данного отчета

## Формирование метрики
В качестве метрики было решено использовать время создания результирующего отчета (`result.json`)
в секундах  
В качестве входных данных используется файл `data_large.txt`, размером ~ 130 МБ.
Бюджет был определен заказчиком и составляет 30 секунд

## Гарантия работы оптимизированной программы
В программе уже есть тест, выполнение которого теста в фидбек-лупе, созданном для тестирования,
позволяет не допустить изменения логики программы при оптимизации. В случае необходимости с 
помощью библиотеки `RSpec` будут написаны дополнительные тесты

## Формирование ассимптомтики
Для формирования ассимптотики была использована библиотека `ruby_prof`, для формирования `flat` 
отчета  
Был сформирован файл `data_test.txt`, в который постпепенно добавлялись данные для исследования
эффективности алгоритма  
В результате были получены следующие тестовые данные  
1 200 пользователей ~ 2.2 секунды  
2 400 пользователей ~ 9.6 секунд  
3 800 пользователей ~ 39 секунд  

Исходя из полученных данных был сделан вывод, что алгоритм менее эффективен, чем линейный.
Также предварительно было выявлено, что большую часть процессорного времени занимают такие
методы, как `<Class::Date>#parse`, `Array#map` и `Regexp#match`

## Feedback-Loop
Для оптимизации был выбраны входные данные, содержащие данные для 200 пользователей.
В результате даже с использованием трассирующих профилировщиков сам процесс занимает не более 3-х
секунд  
Для эффективной оптимизации был разработал следующий `feedback-loop`:
Профилирование с помощью ruby_prof - calgrind -- Оптимизация -- 
Проверка работы процессов с помощью тестов -- Написания тестов на производительность  
Это позволило достаточно эффективно оптимизировать процессы в `task-1.rb`

## Поиск точек роста
### Исследование с помощью ruby_prof - graph
Оптимизация была начата с использованием `ruby_prof` с выгрзукой получившихся результатов в
`RubyProf::GraphHtmlPrinter`  
В результате было выявлено, что основную часть процессорного метода занимает 
метод `Object#collect_stats_from_users`, в котором в свою очередь большую часть времени
занимает метод `<Class::Date>#parse`  

### Исследование с помощью ruby_prof - callgrind
Для подтверждения гипотезы об основной точке роста в `<Class::Date>#parse` было решено повторить
исследование с использованием `ruby_prof` с выгрзукой получившихся результатов в
`RubyProf::CallTreePrinter`  
Также было решено немного изменить файл приложения `task-1.rb` для выключения garbage collector'a
в процессе оптимизации  
В результате данного исследования было подтверждено, что основная точка роста - метод 
`<Class::Date>#parse`  

### Оптимизация работы с датами
В исходных данных даты уже предоставлены в необходимом формате, поэтому необходмости переводить
их в нужный формат еще раз нет. Таким образом от `<Class::Date>#parse` можно избавиться.  
В результате избавления от данного метода callgrind репорт показал прирост в два раза.  
Также был написан тест для закрепления результатов оптимизации с помощью библиотеки RSpec benchmark

### Дальнейшая оптимизация
Так как желаемых результатов достигнуто не было, оптимизация была продолжена  
В тестовый набор данных было добавлено 200 пользователей (таким образом, в тестовом наборе данных
теперь 400 пользователей) для возвращения времени работы процесса до ~ 2-х секунд
Дальнейшее исследование показало, что большую часть времени занимает метод `Array::map`, 
вызывающийся из `Object#collect_stats_from_users`  
Так как в данный метод передаются различные блоки, то было решено временно разделить данный метод
на много одинаковых методов для понимания того, какие блоки работают медленнее всего.  
В результате было выяснено, что медленнее всего работают блоки по:  
1 Сбору статистики по браузером пользователей  
2 Сбору статистики по времени пользователей  
3 Сбору статистики по самой динной сессии пользователя  
Было решено начать оптимизацию со статистики по браузерам.  

### Оптимизация сбора статистики по браузерам пользователей
В отчете необходимо показывать браузеры пользователя в верхнем регистре. Для этого по коллекции
браузеров итерировались два раза. Это было исправлено, в результате чего время формирования
отчета сократилось на 4 процента.

### Оптимизация сбора статистики по самой длинной сессии пользователя
В отчете необходимо выводить самую длинную сессиюю по пользователю. Для этого несколько раз
итерировались по коллекции, что и было поправлено. В результате время формирования отчета
сократилось на 6 процента.

### Оптимизация сбора статистики времени
В отчете необходимо выводить общее время сессий пользователя. Как и в прошлом случае основной
проблемой были множественные итерации по одной коллекции. Исправление этого уменьшило
время формирования отчета еще на 2 процента.

### Дальнейшая оптимизация сбора статистики по браузерам пользователей
Дальнейшее исследование сбора статистики по браузером пользователей обнаружило использование метода
`String::upcase` для названий браузеров во многих участках программы. было решено преобразовывать
названия браузеров в строки в верхнем регистре на этапе парсинга файла, что привело к сокращению
времени формирования отчета на 20 процентов.

### Дальнейшая оптимизация сбора статистики времени и по самой длинной сессии
Используемый инструмент для оптимизации показал, что теперь самое большое время занимают процессы
сбора статистики по времени и по самой длинной сессии. В рамках процесса сбора этих данных
шло преобразование строковых значений в тип `Integer`. Этот процесс был также перенесен на этап
парсинга файла, что позволило сократить время формирования отчета еще на 30 процентов. 

### Оптимизация парсинга файла
Далее была предпринята попытка загрузки большого файла. Недолгое исследование показало, что этап
парсинга займет более четырех дней. Выяснено это было с помощью инстурмента `ProgressBar` на двух
этапах первоначального парсинга. Первый этап был связан с вычленением из файла информации о
пользователях и сессиях, а второй с созданием промежуточной структуры, из которой собирается
статистика о каждом пользователе.  
В результате замены построчного сравнивания на `Array::select` в первом случае и 
замены вызова `Array::select` на каждого пользователя на единичный вызов `Enumerable::group_by`
удалось добиться формирования статистики за 87 секунд при условии использования трассирующего
профилировщика.

### Оптимизации статистики по уникальным браузерам
Дальнейшее исследование показало, что большую часть процессорного времени занимает `Array:all?`,
который вызывается при подсчете общего количества уникальных браузеров. Использования метода
`Array:uniq` позволило значительно сократить время создания отчета, сократив общее время до 57
секунд при условии использования трассирующего профилировщика.

## Результаты
В результате проведенной оптимизации сбор статистики с предоставленным большим объемом входных
данных сократился с неизвестного количества времени (больше 4-х дней) до 52-х секунд. 
В заданный бюджет уложиться не удалось, но формирование статистики многократно ускорилось.

## Защита от регресси производительности
Для защитаы от возможной регрессии производительности в дальнейшем, был написан тест 
с использованием библиотеки `rspec-benchmark`. 