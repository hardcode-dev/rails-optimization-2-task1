# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: смотрел время отработки программы в профилировщике

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
1. Смотрел время отработки методов в ruby-prof. Graph, если метод требовал относительно больших переработок, то переделывал его, прогонял тесты, запускал заново профилировщик, сравнивал время.
2. Если метод требовал небольших изменений, то сравнивал время их работы с помощью Benchmark.ips, тесты.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof. Flat, ruby-prof. Graph, Benchmark.ips

Вот какие проблемы удалось найти и решить

Честно говоря, только в конце записывал сколько было, сколько стало, тк не запомнил, что надо заполнять этот отчет. Смотрел меняются ли данные профилировщика, пропадает ли главная точка роста и шел дальше. Тем не менее.

### Ваша находка №1
- ruby-prof. Graph Метод select
- Поменял логику на group_by
- Точно не напишу, но значительно, если правильно помню, на 16000 записях с 7 секунд до 2
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №2
- ruby-prof. Graph Метод collect_stats_from_users
- Скомбинировал в один хеш, убрал все вложенные циклы
- 
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №3
- ruby-prof. Graph Метод map{}.max
- Поменял на max_by
- На финальном файле загрузка снизилась с 180 сек, до 147 (один раз сравнивал)
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №4
- ruby-prof. Graph Метод time.to_i
- Определил time.to_i при парсинге
- 50000 строк, c 7.8 до 6.1

### Ваша находка №5
- ruby-prof. Graph Переписал парсинг даты
- Изменил parse_session, он повлек изменения и в parse_user, и репорт решил загружать сразу. Удалил модель User.
- C 6 до 0.9 сек. Основной файл со 150 до 90
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №6
- ruby-prof. Graph Метод to_json
- Поменял на Oj
- Основной с 90 до 86 сек
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №7
- ruby-prof. Graph Рефакторинг
- Поменял определение переменных в методы
- Основной с 86 до 74 сек.
- исправленная проблема перестала быть главной точкой роста

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.

Метрика времени по загрузке файла стала 36 секунд. Хотя я поздно о ней узнал.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы семпл из 50000 строк должен отрабатывать не больше чем за 1 секунду


