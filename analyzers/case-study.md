# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было 
понятно, закончит ли она вообще работу за какое-то разумное время.
Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал 
использовать такую метрику: времени выполнения отчета в мс.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы
при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне
получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
- профилировать метод `work` разными профилировщиками
- найти главную точку роста
- оптимизировать ее
- проверить тестом, что ничего не сломал
- замерить время
- закоммитить изменения

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
1. Выполнил `rubocop --require rubocop-performance ./task-1.rb -a`
2. Написал скрипт `analyzers/demo_data/generate_data.rb` для генерации демо данных с разным количеством строк
    - в `LINE_COUNTS` указываем массив с нужным количеством строк
    - в `DATA_LARGE_PATH` указываем путь до файла data_large.txt
    - выполняем `ruby analyzers/demo_data/generate_data.rb`
3. Использовал профилировщики:
   - rbspy
   - ruby-prof
   - stackprof

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Запустил `ruby_prof#flat` для 10000, 20000, 30000 строк:
   ```
   10000 lines - 2.882155  sec.
   20000 lines - 11.021318 sec.
   30000 lines - 25.546953 sec.
   ```
- главная точка роста `Array#select`
- сделал группировку по id пользователя 
  ```
  sessions_by_user_id = sessions.group_by { |f| f['user_id'] }

  users_objects = users.map do |user|
    User.new(attributes: user, sessions: sessions_by_user_id[user['id']])
  end
  ```
- как изменилась метрика
  ```
   10000 lines - 0.363270 sec.
   20000 lines - 0.538642 sec.
   30000 lines - 1.243847 sec.
   ```
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №2
- отчет rbspy показал точку роста в `.split #parse_session и #parse_user`
- т.к. в методе `#work` мы уже делаем сплит, то передаю сразу массив строк
- как изменилась метрика
    ```
   10000 lines - 0.265986 sec.
   20000 lines - 0.315904 sec.
   30000 lines - 0.802636 sec.
   ```
- исправленная проблема перестала быть главной точкой роста


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

