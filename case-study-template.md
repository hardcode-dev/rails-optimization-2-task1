# Case-study оптимизации

## Изменения от себя
Был добавлен rspec, а также убрана проверка minitest в rspec с сохранением логики.

Также, были созданы дополнительные папки (чтобы организовать некую структуру), профилировщики и бенчмарки, которые я задействовал при решение домашнего задания.

Каждый коммит старался описать, чтобы можно было с коммитами и описание ниже представить полную картину решения дом. задания

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
Я создал несколько файлов из основного файла, ограничев их строками - 1000, 2000, 4000, 8000 и 16000

Запуск этих файлов бенчмарками, при текущей реализации скрипта, показал следующие результаты

Время выполнения
```
1000  lines   0.031980
2000  lines   0.100712
4000  lines   0.348666
8000  lines   1.631479
16000 lines   9.393714
```

Итерация в секунду (сравнение) — может не совсем корректное сравнение, но тут мы четко видим, во сколько раз ухудшается ситуация при увеличение входных данных.
Наглядно видно, насколько все плохо
```
1000  lines:       31.7 i/s
2000  lines:       10.3 i/s - 3.07x  (± 0.08)
4000  lines:        2.9 i/s - 11.03x  (± 0.32)
8000  lines:        0.7 i/s - 45.71x  (± 1.12)
16000 lines:        0.1 i/s - 335.14x (± 3.54)
```
Больше всего похоже на кубическую регрессию

Исходя из представленных данных, обработка всего файла займет слишком много времени


## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
Были написаны тесты на время выполнение и кол-во итераций в секунду. Данные тесты не позволяет мне ухудшить ситуацию

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался профайлером ruby-prof.
Использовал в трех возможных "режимах"

Вот какие проблемы удалось найти и решить

### Ваша находка №1
При запуске профайла ruby-prof в «режиме» flat мы увидели

```
%self      total      self      wait     child     calls  name                           location
92.97     12.003    12.003     0.000     0.000     2436   Array#select                   
 4.54     12.861     0.586     0.000    12.275    16010  *Array#each                     
 0.49      0.164     0.064     0.000     0.101    26798   Array#map
```

Для того, чтобы можно было конкретизировать, воспользовался тем же ruby-prof, но уже в режиме Graph.
В данном режиме очень удобно наблюдать все вызовы методов и кол-во затраченного времени в процентах

Уже в этом отчете я выяснил, что проблема кроется в Array#each, а именно Array#select

Если посмотреть код, то видно, что для каждого пользователя каждый раз идет поиск по всему массиву sessions, чтобы найти сессии для конкретного пользователя.

Я решил, что лучшим способом оптимизировать это — изменить формирование массивов пользователей и сессий. А именно каждому пользователю, на момент формирование массива, присваивать сессии. А от массива sessions вовсе отказаться, так как он избыточен

В таком варианте придется изменить users с типа Array на Hash, дабы использовать ключ хеша для айди пользователя. Поиск по ключу Hash происходит куда быстрее, чем перебирать весь массив.

При таком подходе необходимо учитывать подсчет браузеров и кол-во сессий.

После реализации задуманного результаты бенчмарков таковы:
- Время выполнения
```
1000  lines    0.016037
2000  lines    0.038193
4000  lines    0.069304
8000  lines    0.165014
16000 lines    0.340308
```

- Итераций в секунду
```
1000  lines:       54.6 i/s
2000  lines:       26.1 i/s - 2.09x  (± 0.03)
4000  lines:       11.3 i/s - 4.83x  (± 0.09)
8000  lines:        5.6 i/s - 9.73x  (± 0.12)
16000 lines:        2.6 i/s - 20.77x  (± 0.48)
```

По этим данным видно, что ситуация многократно улучшилась.

### Ваша находка №2
Следующее профилирование показало, что следующий, кто много кушает, это Array#each — то есть постоянные вызовы Array#each.

Воспользовавшись ruby-prof в режиме callgrind (с ним удобнее) обнаружил тех, кто вызывает Array#each.

Одним из них, наиболее значимым, был метод collect_stats_from_users. Которого, в свою очередь, за итерацию вызывают аж 7 раз.

Если посмотреть, то метод collect_stats_from_users каждый раз делает проход по всем объектам юзеров через each и так 7 раз. Более того, в нем идет каждый раз перебор user.sessions, а также перебор других данных, что вкупе и делает многократные вызовы Array#each.

Было принято решение, что лучше уместить все вызовы  collect_stats_from_users в один вызов, а в нем за один проход по user.sessions собирать всю необходимую статистику и избавится от лишних вызовов map

После введенных правок, обработка файла стала происходить еще быстрее, хоть прирост и не такой значительный, как в прошлый раз

- Время выполнения
```
1000  lines    0.014312
2000  lines    0.028405
4000  lines    0.058468
8000  lines    0.141335
16000 lines    0.291534
```

- Итераций в секунду
```
1000  lines:       62.4 i/s
2000  lines:       29.7 i/s - 2.10x  (± 0.03)
4000  lines:       13.6 i/s - 4.60x  (± 0.04)
8000  lines:        6.5 i/s - 9.66x  (± 0.13)
16000 lines:        3.2 i/s - 19.34x  (± 0.30)
```

### Ваша находка №3
Следующее профилирование снова показывало на изрядное использование Array#each

На этот раз решил при переборе атрибутов юзеров сразу же поставить сбор статистики, чтобы не приходилось по список юзеров снова проходить.
Также решил избавиться от излишних map при формировании списка браузеров

Все это позволило еще немного ускорить процесс

- Время исполнения
```
1000  lines    0.012624
2000  lines    0.024670
4000  lines    0.047422
8000  lines    0.107396
16000 lines    0.218251
```

- Итераций в секунду
```
1000  lines:       78.9 i/s
2000  lines:       39.7 i/s - 1.99x  (± 0.05)
4000  lines:       17.8 i/s - 4.45x  (± 0.10)
8000  lines:        8.7 i/s - 9.08x  (± 0.21)
16000 lines:        4.4 i/s - 18.09x  (± 0.49)
```

Прирост есть, но этого недостаточно

### Ваша находка №4

Далее, профайлеры жаловались на частые вызовы метода `collect_stats_from_users`, на конвертацию хеша в json, на приведение даты в определенный формат в статистике, на вызов split у строки и на вызов метода `include?` у массива браузеров.

Просмотрев данные, я пришел к выводу, что приведение строки даты в Date и конвертация в формат — лишняя операция, данные у нас в нужном формате.

Также я решил перевести обойтись без использования метода у массива `include?`. Перевел список браузеров в хеш - где ключ - название браузера.

Таким образом, перед добавлением браузера в хеш, мы моментально по ключу можем проверить наличие браузеры, что позволит увеличить скорость по сравниню с поиском по массиву

Все это дало неплохой прирост к производительности

- Время выполнения
```
1000  lines    0.007909
2000  lines    0.010398
4000  lines    0.022536
8000  lines    0.057418
16000 lines    0.110815
```

- Итераций в секунду
```
1000  lines:      157.7 i/s
2000  lines:       79.8 i/s - 1.98x  (± 0.03)
4000  lines:       29.6 i/s - 5.32x  (± 0.11)
8000  lines:       15.0 i/s - 10.48x  (± 0.13)
16000 lines:        7.4 i/s - 21.21x  (± 0.41)
```

### Ваша находка №5
Если запустить профилировщик сейчас, то увидим, что ругается он именно на `String#split`

Решил плотнее заняться split и попробовать сократить его использование

Удалось избавиться от одного лишнего split\`а и проверить тип(user или session) приходящей строки, используя start_with? - можно было просто сделать `line[0] == 'u'` — но разницы в производительности я не заметил

В итоге удалось достичь таких цифр
```
1000  lines    0.005381
2000  lines    0.008395
4000  lines    0.018191
8000  lines    0.041436
16000 lines    0.098116
```

Пока что в бюджет не укладываемся (выполнение всего файла проходит за 37 секунд), но продолажем оптимизировать

### Ваша находка №6

Если смотреть дальше, то проблема все также в `String#split`, `Array#each` и `to_json`.

С последним я ничего не сделаю, но решил попробовать избавиться от дополнительного прохода по юзерам для сборка статистики.

Также, решил убивать ненужных юзеров сразу после сбора статистики и подсчет кол-во юзеров во время выполнения прохода по строкам

Удалось обойтись одним проходом! Все делаем во время чтения строк!

- В итоге, время выполнения
```
1000  lines    0.004346
2000  lines    0.008665
4000  lines    0.019238
8000  lines    0.039046
16000 lines    0.088665
```

- Итераций в секунду
```
1000  lines:      182.6 i/s
2000  lines:       91.6 i/s - 1.99x  (± 0.03)
4000  lines:       44.8 i/s - 4.07x  (± 0.06)
8000  lines:       19.2 i/s - 9.50x  (± 0.23)
16000 lines:       10.1 i/s - 18.04x  (± 0.31)
```

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с несколько дней за полный файл до 26.08 секунд. При выключенном GC - 19.58 с.

Соответственно, удалось уложиться в заданный бюджет

Также, научился пользоваться профилировщиками и прям удивился, как с ними удобно и быстро можно найти проблемы в приложениях

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были заданы новые достигнутые результаты в rspec тестах.

Проходят тесты по времени и итерации в секунду

