# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 

Написал скрипт data_generator.rb, который генерит несколько файлов, содержащих в два раз больше строк чем в предыдущем. Создал work_method_benchmark.rb для оценки асимпотики.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*
Не совсем понял этот момент? Время по какому-то конкретному улучшению?

Вот как я построил `feedback_loop`: benchmark -> profiling -> recognize critical point -> modify -> test -> benchmark -> repeat -> commit

Например как я это делал во втором коммите:

- Выношу строчку с select в отдельный метод get_user_sessions(sessions, user) 
- Пишу тест на этот метод (test_get_user_sessions) 
- Делаю benchmark get_user_sessions.rb  
- Метод get_user_sessions занимает 1.533k i/s   
- Теперь добавляю бинарный поиск на первое совпадение по id и дальше проверяю ближайшие значения слева и справа.   
- Снова делаю benchmark. Теперь показывает ~318.000 i/s. Более чем в ~200 раз быстрее. 
- Снова прогоняю тест test_get_user_sessions 
- Делаю коммит  

Смотрю benchmark для метода #work. Стал быстрее, но асимптотика не изменилась. По такому же принципу продолжаю feedback loop

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
rbspy, stackprof, calltree (qcachegrind), ruby_prof
Каждый из них оказался по-своему полезен.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Profiling показал на select. И я решил попробывать binary searh (200x speed improvement) (см. второй коммит)
### Ваша находка №2
- много оптимизаций в collect_user_stats

### Ваша находка №3
- вместо того чтобы собирать огромный массив с пользователями и сессиями и потом с ним работать, гораздо эффективнее производить все операции во время чтения файла.
Поэтому binary search здесь вообще не причем.

Это сначала помогло оптимизировать время исполнения программы до ~60 секунд, и затем до ~23 секунд с дополнительными оптимизациями.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с того что обработка файла изначально занимала около ~20 дней и после около 23 секунд  что укладывается в заданный бюджет.


*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

Написал тест на linear_performance.

