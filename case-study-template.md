# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: смотрел время отработки программы в профилировщике

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
1. Смотрел время отработки методов в ruby-prof. Graph, если метод требовал относительно больших переработок, то переделывал его, прогонял тесты, запускал заново профилировщик, сравнивал время.
2. Если метод требовал небольших изменений, то сравнивал время их работы с помощью Benchmark.ips, тесты.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof. Flat, ruby-prof. Graph, Benchmark.ips

Вот какие проблемы удалось найти и решить

Честно говоря, только в конце записывал сколько было, сколько стало, тк не запомнил, что надо заполнять этот отчет. Смотрел меняются ли данные профилировщика, пропадает ли главная точка роста и шел дальше. Тем не менее.

### Ваша находка №1
- ruby-prof. Graph Метод select
- Поменял логику на group_by
- Точно не напишу, но значительно, если правильно помню, на 16000 записях с 7 секунд до 2
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №2
- ruby-prof. Graph Метод collect_stats_from_users
- Скомбинировал в один хеш, убрал все вложенные циклы
- 
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №3
- ruby-prof. Graph Метод each (чтение из файла)
- Поменял на take_while(до сих пор не уверен, в нем, но все работает с тестами)
- На 50000 строк с 10 секунд до 1.5
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №4
- ruby-prof. Graph Метод map{}.max
- Поменял на max_by
- На финальном файле загрузка снизилась с 180 сек, до 147 (один раз сравнивал)
- исправленная проблема перестала быть главной точкой роста


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
На семпле даты из 16000 строк, снизилась загрузка с 7 секунд до 0.4 секунд.

Финальный файл все равно грузится долго, есть идея только радикально поменять программу, но не в этом же смысл задания? Также возможно на более мощном компьютере результат будет другой, у меня проц слабоват совсем.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы семпл из 50000 строк должен отрабатывать не больше чем за 8 секунд

