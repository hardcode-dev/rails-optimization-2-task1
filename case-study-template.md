# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время выполнения программы

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 

1) Запускаю файл с тестом. (Вынес файл с тестом в отдельный файл, чтобы можно было запускать каждый раз тест и проверять, что после изменения программа ведет себя как задумано и ничего не поломалось.) 

2) Запускаю профайлер. (Вынес в отдельный файл запуск профайлера с автоматическим запуском выводов профайлера)

3) Нахожу точку роста и думаю как это можно исправить.

3) После исправления запускаю бенчмарк и смотрю на результат. Записываю полученное время. (Вынес в отдельный файл запуск бенчмарка)

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Находка №1 Метод select
Заменил метод select на формирование хеша "id юзера" = " массив сессий конкретного юзера", теперь можно выбрать все сессии юзера практически мгнновенно, Результат выполнения программы 
Запуск на 32000 тысячах записей проходил до улучшения - 66 секунд, после 5-6 секунд.
### Находка №2 Метод each
После запуска профайлера стало понятно что следующая точка метод each в методе collect_stats_from_users. 
Первая мысль заменить метод each на цикл while - результат не изменился, вернул все обратно
После некоторого залипания стало понятно что по одному и тому же юзеру снова и снова в блоках происходят одни и те же итерации. Решил переделать collect_stats_from_users без вызова блока и вызвать данный метод один раз - улучшилось на 0.5 секунды

### Находка №3 Date.parse
Решил перейти к функции Date.parse внутри each. После замены на Date.strptime сползла в выводе профайлера со второй строчки на пятую, время на 32000 тысячах сократилось на 0.2 секунды, но заметно на 64000 сократилось на 0.8 секунд (с 6.1 секунд до 5.24), то есть время выполнения на 64 приблизилось к 32000 тысячам.
И так время выполения на 32000 строк - 5.09 секунды, на 64000 строчках - 5.15 секунды.
Понял что метод iso8601 можно не применять и можно вообще не парсить:) Переделал стало еще быстрее.

### Находка №4 метод split в parse session
Основной тормоз метода each(который в данный момент является основным тормозом программы) - split(',') Попытка написать самописный сплит через each_char оказалась не удачной. Время выполнения даннйо части выросло на 30%. Откатился обратно. 
После некоторого наблюдения, дошло что уже сплитится в методе выше и по cols[0] вычисляется, какой метод будет парсить. Теперь передаю cols сразу в методы парсеры

### Находка №5 замена map внутри collect_stats_from_users
Заменил все map на один each, внутри которого собрал все браузеры даты и время в соответсвующие массивы, результат на 64000 строках 4.99,
метод collect_stats_from_user ушел на вторую строку перехожу на метод data_init

TOTAL    (pct)     SAMPLES    (pct)     FRAME
142     (44.1%)       77    (23.9%)     Object#data_init
75      (23.3%)       75    (23.3%)     Object#collect_stats_from_users

### Находка №6 внутри data_init
вот что имеем:

                                  |    64  | def data_init(file_lines)
                                  |    65  |   users = []
                                  |    66  |   sessions = []
  142   (44.1%)                   |    67  |   file_lines.each do |line|
                                  |    68  |     cols = line.split(',')
   97   (30.1%) /    75  (23.3%)  |    69  |     users << parse_user(cols) if cols[0] == 'user'
   44   (13.7%) /     1   (0.3%)  |    70  |     sessions << parse_session(cols) if cols[0] == 'session'
    1    (0.3%) /     1   (0.3%)  |    71  |   end
                                  |    72  |   { users: users, sessions: sessions }
из этого понятно что надо копать в parse_user
а там особо не покопаешь...:

def parse_user(cols)
  {
    'id' => cols[1],
    'first_name' => cols[2],
    'last_name' => cols[3],
    'age' => cols[4]
  }
end

### Решил переписать
  Переписал за один проход, все тоже самое после того как переписал снова проходил по фидбек-луп, все было похоже на то что уже описал сверху, кроме улучшения сериалайзинга json, после подключения гема oj этот кусок с 26% процентов упал до 2.6%
  (2.6%) /     4   (2.6%)  |    74  |   File.write('jsons/result.json', "#{Oj.dump(report)}\n")
 и в результате большой файл обработался за 20.691 сек.
###  

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с неизвестного мне времени(на второй лекции звучала цифра в 24 дня, я график тоже построил но не знал что он мапиться на эту функцию) и уложиться в заданный бюджет 30 секунд.


## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

