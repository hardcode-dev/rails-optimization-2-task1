# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Первым делом провел бенчмарки на меньшем объёме данных:

- 5000 строк - 0.53с.
- 10000 строк - 2.59с.
- 25000 строк - 20.36с.
- 50000 строк - 78.19с.
- 100000 строк - 275.12c.

Рост нелинейный, наша задача привести эти метрики к линейному виду.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 10-20 сек.

Вот как я построил `feedback_loop`:
 - Нарезал данные на куски меньшего объёма
 - После каждого изменения - запуск бенчмарка и анализ

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
 - rbspy
 - ruby-prof

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Попробовал сделать быструю поверхностную оценку через rbspy. Получил информацию о том, что 99% времени уходит на метод work. Информации маловато.
- Посмотрел flat report при помощи ruby-prof. 89% времени уходит на Array#select
- Посмотрел на структуру получаемых данных и изучил код. В данных, которые мы получаем явно задана последовательность между юзером и сессиями, которую мы никак не учитываем и обрабатываем входящие данные, как случайную последовательность
- Пользуясь знаниями о структуре данных можем сделать вывод о том, что нам не нужно отдельно сохранять юзеров и сессии и потом сопоставлять друг с другом, достаточно грамотно обработать входные данные
- Избавившись от структур users и sessions и оставив только user_objects, в которую мы сразу же пишем входные данные получили следующие результаты:
- 5000 строк - 0.13с.
- 10000 строк - 0.24с.
- 25000 строк - 0.52c.
- 50000 строк - 1.10c.
- 100000 строк - 2.62c.
- Нам удалось перейти к линейному росту. Главная точка роста устранена.

### Ваша находка №2
- Тем же способом через flat report определил следующую точку роста Array#all? Заодно попробовал graph и callstack, увидел аналогичную картину
- Очевидно, что проверка if uniqueBrowsers.all? { |b| b != browser } не оптимальна, заменил на unless uniqueBrowsers.include?(browser)
- Получили следующий результат
- 5000 строк - 0.08с.
- 10000 строк - 0.18с.
- 25000 строк - 0.43c.
- 50000 строк - 0.91c.
- 100000 строк - 2.15c.
- Показатели улучшились, главная точка устранена.

### Ваша находка №X
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

