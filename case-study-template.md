# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
среднее время выполнения для файла длинной в 10к строк.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~5 секунд

Вот как я построил `feedback_loop`:
1) измерил текущие значение метрики через benchmark-ips
2) прикинул "бюджет" метрики для конкретной итерации
3) написал простой перформанс тест, который валится если реальное время выполнения дольше номинального
4) написал простой перформанс тест, который валится если мы не укладываемся в бюджет
Между находками ранил benchmark-ips руками чтобы точнее вычислить номинальное значение метрики перед следующей оптимизацией. benchmark-ips слишком медленный для включения в основной `feedback_loop`

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- ruby-prof для профайлинга и генерации детальных отчетов по времени выполнения
- benchmark-ips - более точное среднее время выполнения
- rubocop-performance/fasterer - защита от регрессии: линтинг/статик анализ кода
- minitest (+ встроенный в руби Benchmark) - быстрый feedback loop

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- После того как я разбил #work на несколько методов для более подробного профайлинга, главную точку роста показал callgrid отчет (реальное время выполнения):
больше всего времени уходит на итерацию по массивам, после более детального профайлинга оказалось что это вызвано медленным алгоритмом для маппинга сессий к юзерам (O(n^2)).
- Я использовал другую структуру данных для хранения коллекции сессий - Hash позволяет найти все сессии юзера за (условно) константное время. Т.е. время выполнения стало O(n).
- 5.6 -> 0.13 секунд 
- большая часть време все еще уходит на Array#each, но новая главная точка роста - Array#all?

### Ваша находка №2
- Самую детальную инфу показал callgrid. Главная точка расто Array#all? который используется для формирования списка уникальных браузеров сессий.
- Это снова алгоритмическая проблема - в худшем случае, если все браузеры уникальны мы имеем O(n^2) итераций. Было принято использовать структуру данных, которая поддерживает уникальность элементов за константное время. Между Hash и Set был выбран Set.
- 0.13 -> 0.09 секунд 
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №3
- Call stack report, главная точка - Date.parse
- Даты уже идут в нужном нам формате и верно сортируются как строки - поэтому я избавился от Date.parse
- 0.09 -> 0.05 секунд
- эта проблема перестала быть главной точкой роста

### Ваша находка №N
Undocumented, но подход работает)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными за ~24 секунды.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был добавлен перформанс тест и линтинг

