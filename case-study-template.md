# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время обработки тестового файла, размером 20000 строк.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~10 секунд

Вот как я построил `feedback_loop`: подготовил файл, обработка которого выполняется за приемлемое время и написал тест измеряющий время обработки этого файла

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался: 
профилировщиками RBSPY, 

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Отключение GC приводит к увеличению времени обработки и не приводит к "стабильному" времени выполнения
Прошелся по коду performance чекерами (rubocop-performance и fasterer), поправил замечания.

### Ваша находка №2
Запустил профилировщик RBSPY при обработке большого файла, обратил внимание что скрипт не проходит дальше разбиения строки на пользователей и сессии. 
Внес немного оптимизаций - функция split вызывалась многократно.
На тестовых данных это улучшение почти не показало результатов.

### Ваша находка №3
Сгенерировал Flat отчет из ruby-prof, обратил внимание что 98% времени занимает выборка сессий при формировании обьектов пользователей.
Зарефакторил эту логику - перенес создание обьекта пользователя в блок парсинга строк и стал собирать сессии в хеш по ключу user_id
Это помогло значительно ускорить обработку файла на 20000 строк - с ~6 секунд до ~0.5

### Ваша находка №4
Снова сгенерировал Flat отчет, на этот раз самое значительное время (34%) заняла проверка уникальности браузера
Перенес сбор этой информации в блок парсинга (использовал класс Set)
Обработка файла на 100000 строк ускорилась с ~2.2 секунд до ~1.7

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
